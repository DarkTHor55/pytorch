# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gE_h-EmA-_OIRACyzBq1IL8yWpgHP-og
"""

def dy_dx(x):
  return 2*x

dy_dx(2)

import torch
x=torch.tensor(3.0,requires_grad=True)

y=x**2

x

y

y.backward()

x.grad

y.grad

import math
def dz_dx(x):
  return 2*x*math.cos(x**2)

dz_dx(4)

x=torch.tensor(4.0,requires_grad=True)

y=x**2

z=torch.sin(y)

x

y

z

z.backward()

x.grad

x=torch.tensor(6.7)
y=torch.tensor(0.0)

w=torch.tensor(1.0,requires_grad=True)
b=torch.tensor(0.0,requires_grad=True)

z=w*x+b
z

y_pred=torch.sigmoid(z)
y_pred

# Binary Cross-Entropy Loss for scalar
# L = -( y * log(y_hat) + (1 - y) * log(1 - y_hat) )

def binary_cross_entropy_loss(prediction, target):
   epsilon=1e-8
   prediction=torch.clamp(prediction,epsilon,1-epsilon)
   return -(target*torch.log(prediction)+(1-target)*torch.log(1-prediction))

loss=binary_cross_entropy_loss(y_pred,y)
loss

loss.backward()

w.grad

b.grad

x=torch.tensor([1.0,2.0,3.0],requires_grad=True
               )

y=(x**2).mean()

y.backward()

x.grad

x.grad.zero_()

x = torch.tensor(2.0, requires_grad=True)
x

y = x ** 2
y

y.backward()

x.grad

x.requires_grad_(False)

# option 1 - requires_grad_(False)
# option 2 - detach()
# option 3 - torch.no_grad()

# z = x.detach()